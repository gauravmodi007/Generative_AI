{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a476ffe7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-29T01:01:40.249747Z",
     "iopub.status.busy": "2024-04-29T01:01:40.248740Z",
     "iopub.status.idle": "2024-04-29T01:01:41.183022Z",
     "shell.execute_reply": "2024-04-29T01:01:41.181767Z"
    },
    "papermill": {
     "duration": 0.949903,
     "end_time": "2024-04-29T01:01:41.186661",
     "exception": false,
     "start_time": "2024-04-29T01:01:40.236758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/rag-pdf/Investment Case For Disruptive Innovation.pdf\n",
      "/kaggle/input/questions/Evaluation_Questions.txt\n",
      "/kaggle/input/ark-data/ARK.md\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e6b1a",
   "metadata": {
    "papermill": {
     "duration": 0.00967,
     "end_time": "2024-04-29T01:01:41.207610",
     "exception": false,
     "start_time": "2024-04-29T01:01:41.197940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Strategy for RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1b6fe",
   "metadata": {
    "papermill": {
     "duration": 0.01016,
     "end_time": "2024-04-29T01:01:41.228072",
     "exception": false,
     "start_time": "2024-04-29T01:01:41.217912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Given PDF Document is very complex unstructured data, so I will use Llama Index, Llama Parser libraries and packages to convert this PDF document into markdown document. \n",
    "* I can parse each section of the markdown document for each and every question but it's time consuming and not the objective of this task, I will use this markdown document to build a good RAG system. \n",
    "* From langchain unstructured markdown document loader, load the document, split document in smaller chunks, convert these chunks in embeddings and store their embeddings into a vector database. \n",
    "* There are many algorithms and tools available to retrieve related information from vector batabse, but for time, processing power, memory and cost constraints, I will use a simple but effective maximum margin relevance algorithm. \n",
    "* I will google Flan t5 LLM model to generate the response. \n",
    "* There are many methods and techniques available to get accurate, more related answers from LLM. I am planning to use Stuff, Refine, Map Reduce, Map Rerank to feed LLM to generate better answers. \n",
    "* For evaluation purpose I am planning to use RAGAs library, so we can get deeper insight about model performance and accuracy, we will get 4 matrix score (faithfulness, answer_relevancy, context_recall, context_precision) for each answer, also this approach use another LLM to evaluate this so no human bias. \n",
    "* If time permits, I would like to try few models such as Llama3, ChatGPT 3.5 Turbo, Phi3  and few more models from hugging face, to build leaderboard so we can choose very best model \n",
    "* Last but not least, I would like to build a research agent who can get information from the internet and feed into LLM using React, COT and Plan and Execute concepts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b66ed2c",
   "metadata": {
    "papermill": {
     "duration": 0.009818,
     "end_time": "2024-04-29T01:01:41.248974",
     "exception": false,
     "start_time": "2024-04-29T01:01:41.239156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Document Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6101e7f5",
   "metadata": {
    "papermill": {
     "duration": 0.010152,
     "end_time": "2024-04-29T01:01:41.270375",
     "exception": false,
     "start_time": "2024-04-29T01:01:41.260223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Given PDF document is very complex document, Simple PDF loader will not give good results so I am using LlamaParse service to handle complex PDF document and convert to Markdown, so we can get most of text data, there are ways to navigate through each and every section of markdown and get exact information, but because of time constrain, I will just use markdown text.  \n",
    "\n",
    "LlamaParse: Proprietary parsing for complex documents with embedded objects such as tables and figures. LlamaParse directly integrates with LlamaIndex ingestion and retrieval to let you build retrieval over complex, semi-structured documents. It is promised to be able to answer complex questions that simply weren’t possible previously.\n",
    "\n",
    "LlamaParse is the world's first genAI-native document parsing platform - built with LLMs and for LLM use cases.\n",
    "\n",
    "Your LLM application performance is only as good as your data. The main goal of LlamaParse is to parse and clean your data, ensuring that it's good quality before passing to any downstream LLM use case such as advanced RAG.\n",
    "\n",
    "To Execute below code, you need API Key from Llama Index. \n",
    "You get 1k free pages a day. If you sign up for the paid plan, you get 7k free pages a week, and then $0.003 for each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c25e1f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T01:01:41.292800Z",
     "iopub.status.busy": "2024-04-29T01:01:41.292292Z",
     "iopub.status.idle": "2024-04-29T01:02:10.369142Z",
     "shell.execute_reply": "2024-04-29T01:02:10.367273Z"
    },
    "papermill": {
     "duration": 29.090442,
     "end_time": "2024-04-29T01:02:10.371284",
     "exception": true,
     "start_time": "2024-04-29T01:01:41.280842",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mError while parsing the file '/kaggle/input/rag-pdf/Investment Case For Disruptive Innovation.pdf': Failed to parse the file: {\"detail\":\"Invalid authentication token\"}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_parse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaParse\n\u001b[1;32m     14\u001b[0m document \u001b[38;5;241m=\u001b[39m LlamaParse(api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXXXXX\u001b[39m\u001b[38;5;124m\"\u001b[39m, result_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/rag-pdf/Investment Case For Disruptive Innovation.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdocument\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1000\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(document[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)  \u001b[38;5;66;03m##our markdown document from complex PDF\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m## Sample Code through navigate through Markdown Structure and get exact answer for given question \u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index llama-parse\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "\n",
    "embed_model = OpenAIEmbedding(model = \"text-embedding-3-small\")\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "Settings.llm = llm\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "document = LlamaParse(api_key = \"XXXXX\", result_type=\"markdown\").load_data(\"/kaggle/input/rag-pdf/Investment Case For Disruptive Innovation.pdf\")\n",
    "\n",
    "print(document[0].text[0:1000])\n",
    "print(document[0].text)  ##our markdown document from complex PDF\n",
    "\n",
    "## Sample Code through navigate through Markdown Structure and get exact answer for given question \n",
    "\"\"\"\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "node_parser = MarkdownElementNodeParser(llm = OpenAI(model=\"gpt-3.5-turbo-0125\"),num_workers=8)\n",
    "nodes = node_parser.get_nodes_from_documents(document)\n",
    "base_nodes ,objects = node_parser.get_nodes_and_objects(nodes)\n",
    "recursive_index = VectorStoreIndex(nodes=base_nodes+objects)\n",
    "queryengine = recursive_index.as_query_engine(similarity_top_k=25)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39527af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:18:50.782081Z",
     "iopub.status.busy": "2024-04-28T23:18:50.781499Z",
     "iopub.status.idle": "2024-04-28T23:18:50.795297Z",
     "shell.execute_reply": "2024-04-28T23:18:50.794137Z",
     "shell.execute_reply.started": "2024-04-28T23:18:50.782045Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get Questions in List so we can loop through it and prepare Evaluation dataset\n",
    "my_file = open(\"/kaggle/input/questions/Evaluation_Questions.txt\", \"r\") \n",
    "data = my_file.read() \n",
    "data_into_list = data.split(\"\\n\") \n",
    "data_into_list = list(filter(None, data_into_list))\n",
    "my_file.close() \n",
    "questions = [sub[5:] for sub in data_into_list]\n",
    "print(questions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe82201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:19:03.110770Z",
     "iopub.status.busy": "2024-04-28T23:19:03.109164Z",
     "iopub.status.idle": "2024-04-28T23:22:02.419470Z",
     "shell.execute_reply": "2024-04-28T23:22:02.417649Z",
     "shell.execute_reply.started": "2024-04-28T23:19:03.110723Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain\n",
    "!pip install -q tiktoken\n",
    "!pip install -q chromadb\n",
    "#!pip install -q faiss-cpu \n",
    "#!pip install -q openai\n",
    "!pip install -q sentence_transformers\n",
    "!pip install -q ragas\n",
    "!pip install -q evaluate\n",
    "!pip install -q unstructured langchain_community langchain-text-splitters\n",
    "\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "#from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "import evaluate \n",
    "\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218a95d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:24:34.178150Z",
     "iopub.status.busy": "2024-04-28T23:24:34.177126Z",
     "iopub.status.idle": "2024-04-28T23:24:49.024415Z",
     "shell.execute_reply": "2024-04-28T23:24:49.022991Z",
     "shell.execute_reply.started": "2024-04-28T23:24:34.178110Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading Markdown document\n",
    "loader = UnstructuredMarkdownLoader(\"/kaggle/input/ark-data/ARK.md\")\n",
    "documents = loader.load()\n",
    "\n",
    "# using local model, we can use ChatGPT 3.5 Turbo, 4, Llama but because of API cost $, Memory Constrain\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Fill\"\n",
    "cache_dir = \"./cache\"\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, cache_folder=cache_dir) \n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "chunk_size=1000, chunk_overlap=100, separator=\"\\n\")\n",
    "#chunk_size=2000, chunk_overlap=200, separator=\"\\n\")\n",
    "docs = text_splitter.split_documents(documents=documents)\n",
    "vectordb = Chroma.from_documents(docs, embeddings, persist_directory=cache_dir)\n",
    "\n",
    "query = \"What are the significant risks associated with investing in innovation as highlighted by ARK?\"\n",
    "results_with_scores = vectordb.similarity_search_with_score(query,k=5)\n",
    "for doc, score in results_with_scores: print(f\"Content: {doc.page_content}\\nMetadata: {doc.metadata}\\nScore: {score}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653ea96",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67fd9aa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "* Hierarchical Chunking\n",
    "* FlagEmbeddingReranker\n",
    "* RankGPTRerank\n",
    "* Maximal Marginal Relevance: MMR tries to reduce the redundancy of results while at the same time maintaining query relevance of results for already ranked documents/phrases etc. \n",
    "\n",
    "There are many ways and algorithms are available to retirve information from vector databse, I am using MMR for example here, for professional work we can try different algorithms and evaluate performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003b6aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:25:22.309227Z",
     "iopub.status.busy": "2024-04-28T23:25:22.308131Z",
     "iopub.status.idle": "2024-04-28T23:25:52.787165Z",
     "shell.execute_reply": "2024-04-28T23:25:52.785639Z",
     "shell.execute_reply.started": "2024-04-28T23:25:22.309184Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# We want to make this a retriever, so we need to convert our index.  \n",
    "# This will create a wrapper around the functionality of our vector database \n",
    "# so we can search for similar documents/chunks in the vectorstore and retrieve the results:\n",
    "retriever = vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 4, \"lambda_mult\": 0.1})\n",
    "#retriever = vectordb.as_retriever()\n",
    "# This chain will be used to do QA on the document. We will need\n",
    "# 1 - A LLM to do the language interpretation\n",
    "# 2 - A vector database that can perform document retrieval\n",
    "# 3 - Specification on how to deal with this data\n",
    "\n",
    "hf_llm = HuggingFacePipeline.from_model_id(model_id=\"google/flan-t5-large\",task=\"text2text-generation\",model_kwargs={\"do_sample\":True,\"max_length\": 2048,\"cache_dir\": cache_dir,},)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694155e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Answer Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624982b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "* Create and Refine strategy\n",
    "* Hierarchical Summarization Strategy\n",
    "* LangChain Types \n",
    "    1.     STUFF:  We just put all of it into one prompt and send that to the language model and get back one response.\n",
    "    1.     MAP REDUCE: This takes all the chunks, passes them along with the query to a language model, gets back a response, and then uses another language model call to summarize all of the individual responses into a final answer.\n",
    "    1.     REFINE: “Refine”, which is another method, is again used to loop over many documents. But it does it iteratively.It builds upon the response from the previous document\n",
    "    1.     MAP_RERANK: “Map_rerank” is a pretty interesting and a bit more experimental one where we do a single call to the language model for each document. And we also ask it to return a score.And then we select the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4dae3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "There are many algorithms, agents available from Self Reflection, React, Chain of Thought, Plan and Execute, I will use 4 various Chain Types to improve coherence, faithfulness, and relevance of answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba00ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:26:00.259355Z",
     "iopub.status.busy": "2024-04-28T23:26:00.256043Z",
     "iopub.status.idle": "2024-04-28T23:26:00.290208Z",
     "shell.execute_reply": "2024-04-28T23:26:00.288880Z",
     "shell.execute_reply.started": "2024-04-28T23:26:00.259078Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "#llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"refine\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0e4e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:26:03.656511Z",
     "iopub.status.busy": "2024-04-28T23:26:03.654638Z",
     "iopub.status.idle": "2024-04-28T23:26:21.520642Z",
     "shell.execute_reply": "2024-04-28T23:26:21.519500Z",
     "shell.execute_reply.started": "2024-04-28T23:26:03.656402Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ea3dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:26:44.008731Z",
     "iopub.status.busy": "2024-04-28T23:26:44.008195Z",
     "iopub.status.idle": "2024-04-28T23:27:46.098172Z",
     "shell.execute_reply": "2024-04-28T23:27:46.097025Z",
     "shell.execute_reply.started": "2024-04-28T23:26:44.008689Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13768d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:27:46.101296Z",
     "iopub.status.busy": "2024-04-28T23:27:46.100570Z",
     "iopub.status.idle": "2024-04-28T23:28:04.346782Z",
     "shell.execute_reply": "2024-04-28T23:28:04.345550Z",
     "shell.execute_reply.started": "2024-04-28T23:27:46.101255Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d81f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:29:58.020087Z",
     "iopub.status.busy": "2024-04-28T23:29:58.019541Z",
     "iopub.status.idle": "2024-04-28T23:30:19.249392Z",
     "shell.execute_reply": "2024-04-28T23:30:19.248507Z",
     "shell.execute_reply.started": "2024-04-28T23:29:58.020048Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8924f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:30:26.639184Z",
     "iopub.status.busy": "2024-04-28T23:30:26.638725Z",
     "iopub.status.idle": "2024-04-28T23:30:54.791072Z",
     "shell.execute_reply": "2024-04-28T23:30:54.789698Z",
     "shell.execute_reply.started": "2024-04-28T23:30:26.639148Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60b52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:31:01.649419Z",
     "iopub.status.busy": "2024-04-28T23:31:01.648865Z",
     "iopub.status.idle": "2024-04-28T23:31:45.074432Z",
     "shell.execute_reply": "2024-04-28T23:31:45.073229Z",
     "shell.execute_reply.started": "2024-04-28T23:31:01.649376Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2526d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:31:49.282642Z",
     "iopub.status.busy": "2024-04-28T23:31:49.282220Z",
     "iopub.status.idle": "2024-04-28T23:32:41.647902Z",
     "shell.execute_reply": "2024-04-28T23:32:41.646379Z",
     "shell.execute_reply.started": "2024-04-28T23:31:49.282608Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb268a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:32:47.908451Z",
     "iopub.status.busy": "2024-04-28T23:32:47.907995Z",
     "iopub.status.idle": "2024-04-28T23:33:23.238637Z",
     "shell.execute_reply": "2024-04-28T23:33:23.237161Z",
     "shell.execute_reply.started": "2024-04-28T23:32:47.908414Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243fd55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:33:40.026626Z",
     "iopub.status.busy": "2024-04-28T23:33:40.026181Z",
     "iopub.status.idle": "2024-04-28T23:34:04.293077Z",
     "shell.execute_reply": "2024-04-28T23:34:04.291935Z",
     "shell.execute_reply.started": "2024-04-28T23:33:40.026590Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef9b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:34:21.119141Z",
     "iopub.status.busy": "2024-04-28T23:34:21.118728Z",
     "iopub.status.idle": "2024-04-28T23:34:56.314680Z",
     "shell.execute_reply": "2024-04-28T23:34:56.313296Z",
     "shell.execute_reply.started": "2024-04-28T23:34:21.119110Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0408375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:34:56.318613Z",
     "iopub.status.busy": "2024-04-28T23:34:56.318015Z",
     "iopub.status.idle": "2024-04-28T23:35:22.439278Z",
     "shell.execute_reply": "2024-04-28T23:35:22.438193Z",
     "shell.execute_reply.started": "2024-04-28T23:34:56.318579Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c552340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T23:35:22.441483Z",
     "iopub.status.busy": "2024-04-28T23:35:22.440957Z",
     "iopub.status.idle": "2024-04-28T23:35:34.972824Z",
     "shell.execute_reply": "2024-04-28T23:35:34.971833Z",
     "shell.execute_reply.started": "2024-04-28T23:35:22.441450Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e088615",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T00:04:38.375042Z",
     "iopub.status.busy": "2024-04-29T00:04:38.374585Z",
     "iopub.status.idle": "2024-04-29T00:05:03.125751Z",
     "shell.execute_reply": "2024-04-29T00:05:03.124787Z",
     "shell.execute_reply.started": "2024-04-29T00:04:38.375009Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e879df9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T00:05:37.987409Z",
     "iopub.status.busy": "2024-04-29T00:05:37.986175Z",
     "iopub.status.idle": "2024-04-29T00:06:07.229604Z",
     "shell.execute_reply": "2024-04-29T00:06:07.228176Z",
     "shell.execute_reply.started": "2024-04-29T00:05:37.987360Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe3e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T00:06:07.233069Z",
     "iopub.status.busy": "2024-04-29T00:06:07.232286Z",
     "iopub.status.idle": "2024-04-29T00:06:59.829905Z",
     "shell.execute_reply": "2024-04-29T00:06:59.828625Z",
     "shell.execute_reply.started": "2024-04-29T00:06:07.233015Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb07746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T00:06:59.832689Z",
     "iopub.status.busy": "2024-04-29T00:06:59.831892Z",
     "iopub.status.idle": "2024-04-29T00:07:42.887778Z",
     "shell.execute_reply": "2024-04-29T00:07:42.886378Z",
     "shell.execute_reply.started": "2024-04-29T00:06:59.832640Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45baefa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T00:07:42.892681Z",
     "iopub.status.busy": "2024-04-29T00:07:42.890706Z",
     "iopub.status.idle": "2024-04-29T00:08:04.583506Z",
     "shell.execute_reply": "2024-04-29T00:08:04.582379Z",
     "shell.execute_reply.started": "2024-04-29T00:07:42.892632Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbab4d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T00:08:04.585380Z",
     "iopub.status.busy": "2024-04-29T00:08:04.584939Z",
     "iopub.status.idle": "2024-04-29T00:08:42.734671Z",
     "shell.execute_reply": "2024-04-29T00:08:42.733205Z",
     "shell.execute_reply.started": "2024-04-29T00:08:04.585336Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd962f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T00:08:42.736358Z",
     "iopub.status.busy": "2024-04-29T00:08:42.736017Z",
     "iopub.status.idle": "2024-04-29T00:09:00.281526Z",
     "shell.execute_reply": "2024-04-29T00:09:00.280097Z",
     "shell.execute_reply.started": "2024-04-29T00:08:42.736312Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa8b6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T00:09:00.283674Z",
     "iopub.status.busy": "2024-04-29T00:09:00.283219Z",
     "iopub.status.idle": "2024-04-29T00:09:31.083098Z",
     "shell.execute_reply": "2024-04-29T00:09:31.081849Z",
     "shell.execute_reply.started": "2024-04-29T00:09:00.283639Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(questions[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39e57d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepareData(qa): \n",
    "    from datasets import Dataset\n",
    "        #ground_truths = [[\"GT1\"],[\"GT2\"],[\"GT3\"]]\n",
    "    answers = []\n",
    "    contexts = []\n",
    "\n",
    "    # Inference\n",
    "    for query in questions:\n",
    "        answers.append(qa.invoke(query))\n",
    "        contexts.append([docs.page_content for docs in retriever.invoke(query)])\n",
    "\n",
    "    # To dict\n",
    "    data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    #\"ground_truths\": ground_truths\n",
    "    }\n",
    "    # Convert dict to dataset\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69157327",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"refine\", retriever=retriever)\n",
    "dataset_refine = prepareData(qa)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\", retriever=retriever)\n",
    "dataset_stuff = prepareData(qa)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"map_reduce\", retriever=retriever)\n",
    "dataset_map_reduce = prepareData(qa)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"map_rerank\", retriever=retriever)\n",
    "dataset_map_rerank = prepareData(qa)\n",
    "\n",
    "### we can just change the model to ChatGPT 4,3.5 Turbo, Llamma 3, Llamma2, .... and create mode evaluation data sets for leader board. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26116f4a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1aa871",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**RAGAs (Retrieval-Augmented Generation Assessment):** RAGAs is that it started out as a framework for “reference-free” evaluation, That means, instead of having to rely on human-annotated ground truth labels in the evaluation dataset, RAGAs leverages LLMs under the hood to conduct the evaluations.\n",
    "\n",
    "* faithfulness\n",
    "* answer_relevancy \n",
    "* context_recall\n",
    "* context_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1deeac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (faithfulness, answer_relevancy, context_recall, context_precision,)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1ec3b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4890435",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Leader Board**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d98412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:43:45.588114Z",
     "iopub.status.busy": "2024-04-28T15:43:45.587665Z",
     "iopub.status.idle": "2024-04-28T15:43:47.283025Z",
     "shell.execute_reply": "2024-04-28T15:43:47.281641Z",
     "shell.execute_reply.started": "2024-04-28T15:43:45.588080Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "param_grid = {  \n",
    "    'Model_Name': [\"HF_LLM\",\"Llama3\",\"GPT 3.5 Turbo\"],\n",
    "    'Chain_Type': [\"stuff\",\"refine\",\"map reduce\",\"map rerank\"],\n",
    "    }\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "LeaderBoard = pd.DataFrame(all_params)\n",
    "LeaderBoard[['Faithfulness','Answer_Relevancy','Context_Recall','Context_Precision']] = \"\"\n",
    "LeaderBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f90f401",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "320c44d4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Research Agents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775cbaf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "LangChain Agents are very powerful tool to get latest information from external sources and augment with static documents, it can perform various tasks, just one example shown below, how to get infor from internet. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c8ec9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d99b5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint = hf_llm\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_llm)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_llm)\n",
    "pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline = pipeline)\n",
    "tools = load_tools([\"serpapi\"],llm=hf_llm)\n",
    "agent = initialize_agent(tools,llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose= True)\n",
    "agent.run(\"\"\" How do neural networks serve as a catalyst for other technologies? \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4891886,
     "sourceId": 8245556,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4888392,
     "sourceId": 8240684,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4893235,
     "sourceId": 8247474,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34.14658,
   "end_time": "2024-04-29T01:02:11.507023",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-29T01:01:37.360443",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
